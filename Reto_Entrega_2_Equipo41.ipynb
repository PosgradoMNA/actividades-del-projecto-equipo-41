{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "djPe91TyCFJ6",
        "Hp3PkzWzAJ5X",
        "C1LOYc6t8NSH",
        "mAB3GnqvAHlx",
        "TB36iR8IWZfd",
        "rKxy2xkHARfz",
        "-wU0zvsiAkFG",
        "fQMiBtlvAhHD",
        "xhMjiMbfAYeS",
        "8052LpOKAld0",
        "jlXw3sUIAnDi",
        "Zyr3nA06Adv9",
        "IpqPGvPrAvDF",
        "R765Xwv0Ar5f",
        "cjme_nhhAoXJ",
        "ULgzcJdWAiuT",
        "wvwUr2PEApxJ",
        "wXWjPkbUAbuz",
        "T4NtxlRjeJOU",
        "Yx4w6JKFWXyS"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PosgradoMNA/actividades-del-projecto-equipo-41/blob/main/Reto_Entrega_2_Equipo41.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Portada\n",
        "---\n",
        "\n",
        "<br>\n",
        "<p align=center>Instituto Tecnológico y de Estudios Superiores de Monterrey</p>\n",
        "<p align=center>Escuela de Ingeniería y Ciencias</p>\n",
        "<p align=center>\n",
        "<br>\n",
        "<img src=\"https://github.com/PosgradoMNA/actividades-del-projecto-equipo-41/blob/main/tec.png?raw=true\" alt=\"Logo\" width=\"250\"/>\n",
        "</p>\n",
        "<br>\n",
        "<p align=center>Maestría en Inteligencia Artificial Aplicada (MNA)</p>\n",
        "<p align=center>TC4029. Ciencia y Analítica de Datos</p>\n",
        "<p align=center> Profesor Titular: María de la Paz Rico </p>\n",
        "<p align=center> <b>Reto Final - Parte II (Modelo)</b> </p>\n",
        "<br>\n",
        "<p align=center>Presentan:</p>\n",
        "<p align=center>A01150742 | Ovalle Alvarado José</p>\n",
        "<p align=center>A01793023 | Arroyo Chavelas Jorge Luis</p>\n",
        "<p align=center>18 de Noviembre de 2022</p>\n",
        "<br>\n",
        "<br>\n",
        "<p align=center><small><italic>“Por medio de la presente hacemos constar que el reporte que estamos enviando es de nuestra completa autoría y que no estamos haciendo plagio de ideas o escritos del trabajo de otras personas”</italic></small></p>\n",
        "\n",
        "---\n",
        "\n",
        "[GitHub Link](https://github.com/PosgradoMNA/actividades-del-projecto-equipo-41/blob/main/Reto_Entrega_1_Equipo41.ipynb)\n"
      ],
      "metadata": {
        "id": "FTy5JEpgXpC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción"
      ],
      "metadata": {
        "id": "wuYBhsuqXsme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preapremos primero nuestro ambiente de análisis:"
      ],
      "metadata": {
        "id": "VawbDUI0Oedi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Algunas instalaciones previas para Colab:\n",
        "!pip install -q qeds fiona geopandas xgboost gensim folium pyLDAvis descartes"
      ],
      "metadata": {
        "id": "cPBhLf6RQShQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e31a014b-f2bd-4fa5-d8ed-a0615fbdf89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 16.7 MB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 44.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 22.1 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 179 kB 66.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 45.9 MB/s \n",
            "\u001b[?25h  Building wheel for qeds (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################################\n",
        "# Librerias y funciones:\n",
        "############################################################################################\n",
        "\n",
        "# Para obtener los datos\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# Para manipualr datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Para visualizaciones:\n",
        "from tabulate import tabulate\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Para particiones y validaciones\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Para transformaciones:\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Para pipelines:\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Para datos geográficos\n",
        "from shapely.geometry import Point\n",
        "import geopandas as gpd\n",
        "from sklearn.cluster import KMeans\n",
        "from geopy.geocoders import Nominatim\n",
        "from scipy.spatial import ConvexHull\n",
        "from PIL import Image\n",
        "\n",
        "# Otras utilidades\n",
        "import functools\n",
        "import warnings\n",
        "\n",
        "sns.set_theme(style='white', palette=None)\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "iDIMwnHxu33a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtengamos los datos:"
      ],
      "metadata": {
        "id": "NvOnlPV9Onbo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT20qv_sh6Pg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142936d6-593b-4af9-b3fe-ea478fe565a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Dataframe con (1068, 57) dimensiones\n"
          ]
        }
      ],
      "source": [
        "############################################################################################\n",
        "# Llamar los datos:\n",
        "############################################################################################\n",
        "\n",
        "# Definir la url de donde obtener los datos:\n",
        "datos_url = 'http://201.116.60.46/Datos_de_calidad_del_agua_de_5000_sitios_de_monitoreo.zip'\n",
        "\n",
        "# Obtener la respuesta:\n",
        "datos_response = requests.get(datos_url, stream=True)\n",
        "\n",
        "# Tomar respuesta como zip en cache:\n",
        "datos_zip = zipfile.ZipFile(io.BytesIO(datos_response.content))\n",
        "\n",
        "# Extraer todos los archivos del zip en nuestro espacio local:\n",
        "datos_zip.extractall(\"./\")\n",
        "\n",
        "# Definir el lugar donde guardamos los archivos:\n",
        "datos_dir = './Datos_de_calidad_del_agua_2020'\n",
        "\n",
        "# Definir archivos a utilizar\n",
        "datos_file = '/Datos_de_calidad_del_agua_de_sitios_de_monitoreo_de_aguas_subterraneas_2020.csv'\n",
        "\n",
        "# Impotar los datos:\n",
        "df = pd.read_csv(datos_dir + datos_file, encoding = \"cp1252\")\n",
        "\n",
        "# Confirmemos que tenemos el dataset:\n",
        "print(f'\\n Dataframe con {df.shape} dimensiones')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################################\n",
        "# Datos de entrenamiento y prueba:\n",
        "############################################################################################\n",
        "\n",
        "# Conjunto con features\n",
        "X = df.drop('SEMAFORO', axis=1)\n",
        "\n",
        "# Variable objetivo\n",
        "y = df[['SEMAFORO']]\n",
        "\n",
        "# Partir los datos en conjuntos de entrenamiento y prueba:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(f\"\"\"\n",
        "Tamaño de variables de entrenamiento: {X_train.shape} || tamaño de objetivo entrenamiento: {y_train.shape}\n",
        "Tamaño de variables de prueba: {X_test.shape} || tamaño de objetivo prueba: {y_test.shape}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "k4AIlkGeIU6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c481128-7eb3-4eab-f268-831504cece2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tamaño de variables de entrenamiento: (854, 56) || tamaño de objetivo entrenamiento: (854, 1)\n",
            "Tamaño de variables de prueba: (214, 56) || tamaño de objetivo prueba: (214, 1)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}